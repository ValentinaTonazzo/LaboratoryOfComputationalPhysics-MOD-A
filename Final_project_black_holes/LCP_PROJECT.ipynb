{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2103.05016."
      ],
      "metadata": {
        "id": "dGDWuofVckeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical mergers of binary black holes\n",
        "Goal of the project:\n",
        "\n",
        "Understand the differences between hierarchical binary black hole mergers in nuclear star clusters, globular clusters and young star clusters, by looking at a set of simulated binary black holes\n",
        "\n",
        "Tasks:\n",
        "\n",
        "* Plot the main properties of hierarchical black holes in different star clusters. Compare nuclear star clusters, globular clusters and young star clusters\n",
        "\n",
        "* Run some simple machine learning algorithm (e.g., a random forest) to figure out what features have the highest impact on the fate of a binary black hole in the three different kinds of star clusters."
      ],
      "metadata": {
        "id": "xWbX2iZHUgyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONTENTS\n",
        "1. Introduction\n",
        "2. Dataset organization\n",
        "3. Comparative plots of main properties\n",
        "4. PCA analysis\n",
        "5. Random Forest algorithm\n",
        "6. hyperparameters analysis\n",
        "7. Conclusions\n"
      ],
      "metadata": {
        "id": "yozQBZqLUpWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INTRODUCTION\n",
        "A binary black hole can form via close encounters of black holes in a dense stellar environment, such as a nuclear star cluster, a globular cluster or a young star cluster. Nuclear star clusters are very massive (more o less 1e5 – 1e8 solar masses) star clusters lying at the center of some galaxies, including the Milky Way. Globular clusters are old (more or less 12 Gyr) massive (more or less 1e4 – 1e6) stellar clusters lying in the halo of almost every galaxy. Young star clusters are young (< 100 Myr) stellar clusters forming mostly in the disk of a galaxy.\n",
        "\n",
        "#### What are generations?\n",
        "Two black holes may be single objects at birth, and pair up dynamically at some point in their “life”. When two stellar-born black holes merge via gravitational wave emission, their merger remnant is called second-generation (2g) black hole. The 2g black hole is a single object at birth. However, if it is retained inside its host star cluster, it may pair up dynamically with another black hole. This gives birth to what we call a second-generation (2g) binary black hole, i.e. a binary black hole that hosts a 2g black hole . If a 2g binary black hole merges again, it gives birth to a third- generation (3g) black holes, and so on. In this way, repeated black hole mergers in star clusters can give birth to hierarchical chains of mergers, leading to the formation of more and more massive black holes."
      ],
      "metadata": {
        "id": "_gOnYGRUstlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIBRARIES"
      ],
      "metadata": {
        "id": "B8GQmTfj3Hoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from scipy import linalg as la\n",
        "from scipy.spatial import ConvexHull\n",
        "import sklearn as skl\n",
        "import sklearn.metrics as skm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn.preprocessing import StandardScaler#\n",
        "import time\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.tri as mtri\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "%matplotlib inline\n",
        "%matplotlib notebook\n",
        "%pylab"
      ],
      "metadata": {
        "id": "gJ1fF8kr3Kzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3abf824-18e8-4924-d785-c125ec671a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using matplotlib backend: nbAgg\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OUR DATA"
      ],
      "metadata": {
        "id": "28sMuVvGzudZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data was composed by three datasets, each containing informations on binary systems in different types of star clusters. In each dataset, the data was previously organized in columns for an easier analysis. For our study we used the following columns:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   C0: Identifier of the binary.\n",
        "*   C1: Mass of the primary black hole.\n",
        "\n",
        "*   C2: Mass of the secondary black hole.\n",
        "*   C3: Dimensionless spin magnitude of the primary black hole.\n",
        "\n",
        "*   C4: Dimensionless spin magnitude of the secondary black hole.\n",
        "*   C9: Time requested for the dynamical pair up of the system.\n",
        "\n",
        "*   C13: Time elapsed since the first-gen formation until the merger of the nth-gen sytem.\n",
        "*   C15: Mass of the remnant.\n",
        "*   C16: Dimensionless spin magnitude of the remnant black hole.\n",
        "*   C17: Escape velocity from star cluster.\n",
        "\n",
        "*   C25: Total mass of the stelar cluster.\n",
        "*   C27: Number of generation of the system.\n",
        "\n",
        "Additionally, for the machine learning study, the metallicity of the system was added as another column (C28).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xjzjdSTM1lee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting the data"
      ],
      "metadata": {
        "id": "s-Dudb1u_I0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since for each part of the project a different format of the data was more convenient, two similar but different codes were made to extract the data from the files and store it.\n"
      ],
      "metadata": {
        "id": "8Uw_V8Sv_OV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the plotting part it was more useful to store the data on a dataframe. For this reason we read the files we were interested in and concatenate the data of each star cluster into different datasets."
      ],
      "metadata": {
        "id": "5RZa_I6I_sr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder = ['GC_chi01_output_noclusterevolv', 'NSC_chi01_output_noclusterevolv',\n",
        "           'YSC_chi01_output_noclusterevolv']\n",
        "\n",
        "name = 'nth_generation.txt'\n",
        "\n",
        "metallicity = ['0.0002', '0.002', '0.02', '0.0004', '0.004', '0.006', '0.0008',\n",
        "               '0.008', '0.0012', '0.012', '0.0016', '0.016']\n",
        "\n",
        "def choose_set(c,m):\n",
        "    #for this part is important to have the data stored on a folder called 'data'\n",
        "    file = pd.read_csv('data/' + folder[c] + '/Dyn/' + metallicity[m] + '/' + name,\n",
        "                   engine='python', sep = ' ')\n",
        "    df_all = pd.DataFrame(file)\n",
        "    df_all = df_all.drop(columns = [df_all.columns[28]] )\n",
        "\n",
        "    '''\n",
        "    There was an issue with the 13th column name, it has an space on it so pd.read_csv\n",
        "    take that space also as a delimiter for another column. I solved it by deleting the\n",
        "    last column that was full of Nan (since it has nothing to read) and renaming the\n",
        "    columns with 'col_name'\n",
        "    '''\n",
        "    col_name = ['c' + str(i) for i in range(28)]\n",
        "    df_all.columns = col_name\n",
        "\n",
        "    '''\n",
        "    Here it was dropped the columns not bolded\n",
        "    '''\n",
        "    #here i remove the colums with 'ignore them' written on them\n",
        "    df_all = df_all.drop(columns = ['c10', 'c11', 'c12', 'c18', 'c19', 'c20', 'c21',\n",
        "                                'c22', 'c23', 'c24', 'c26'])\n",
        "    #with this we just mantain the bolded ones.\n",
        "    df_filtered = df_all.drop(columns = ['c5', 'c6', 'c7', 'c8', 'c14'])\n",
        "\n",
        "    #As suggested in the methodology, ONLY for the purpose of visualization, we drop also columns 0, 9 and 13\n",
        "    df_filt = df_filtered.drop(columns = ['c0', 'c9', 'c13'])\n",
        "\n",
        "    return df_filt"
      ],
      "metadata": {
        "id": "VSHVcBev3aqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, for the machine learning part, it was more convenient to store all the data together, since the porpuse of this part was to discern the star cluster each system belong to. This was done on a similar way as before, we first stored all the data into dataframes, concatenating them while reading each file. Once all the data was in a single dataframe, it was stored as a numpy array on a .npz file. This way we also avoid to read all the files each time we want to run the code. In addition, while reading the data of each star cluster, another array was made to assign a label to each system. This label array was also stored in the same .npz file."
      ],
      "metadata": {
        "id": "OOLD8ES1A1sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data( nfiles):\n",
        "    name = 'nth_generation.txt'\n",
        "\n",
        "    folder = ['GC_chi01_output_noclusterevolv', 'NSC_chi01_output_noclusterevolv',\n",
        "               'YSC_chi01_output_noclusterevolv']\n",
        "\n",
        "    metallicity = ['0.0002', '0.002', '0.02', '0.0004', '0.004', '0.006', '0.0008',\n",
        "                   '0.008', '0.0012', '0.012', '0.0016', '0.016']\n",
        "\n",
        "    col_name = ['c' + str(i) for i in range(28)]\n",
        "    Y =[]\n",
        "    t1 = time.time()\n",
        "    for i in range(len(folder)):\n",
        "        for j in range(nfiles):\n",
        "\n",
        "            file = pd.read_csv('data/' + folder[i] + '/Dyn/' + metallicity[j] + '/' + name,\n",
        "                               engine='python', sep = ' ')\n",
        "\n",
        "            df_all = pd.DataFrame(file)\n",
        "            df_all = df_all.drop(columns = [df_all.columns[28]] )\n",
        "\n",
        "\n",
        "            df_all.columns = col_name\n",
        "\n",
        "\n",
        "            #here i remove the colums with 'ignore them' written on them\n",
        "            df_all = df_all.drop(columns = ['c10', 'c11', 'c12', 'c18', 'c19', 'c20', 'c21',\n",
        "                                            'c22', 'c23', 'c24', 'c26'])\n",
        "\n",
        "            #with this we just mantain the bolded ones.\n",
        "            df_filter = df_all.drop(columns = ['c5', 'c6', 'c7', 'c8', 'c14'])\n",
        "\n",
        "            #here i define the metallicity\n",
        "            met = pd.Series(np.array([float(metallicity[j]) for i in range(df_all.shape[0])]), name = 'c28')\n",
        "\n",
        "            #here i add the metallicity as another column\n",
        "            df_all = pd.concat([df_all, met],axis = 1)\n",
        "            df_filter = pd.concat([df_filter, met],axis = 1)\n",
        "\n",
        "            #convert df into arrays\n",
        "\n",
        "\n",
        "            df_filter = np.array(df_filter)\n",
        "            df_all = np.array(df_all)\n",
        "\n",
        "\n",
        "            if i == 0 and j==0:\n",
        "                X_all = df_all\n",
        "                X_filter = df_filter\n",
        "\n",
        "            else:\n",
        "\n",
        "                X_all = np.concatenate((X_all, df_all))\n",
        "                X_filter = np.concatenate((X_filter, df_filter))\n",
        "\n",
        "        if i == 0: #label 0 for globular cluster\n",
        "            Y= np.array([0 for k in range(X_filter.shape[0])])\n",
        "        elif i == 1: #label 1 for nuclear star cluster\n",
        "            y = np.array([1 for k in range(X_filter.shape[0]-len(Y))])\n",
        "            Y = np.concatenate((Y, y))\n",
        "        elif i == 2: #label 2 for young star cluster\n",
        "            y = np.array([2 for k in range(X_filter.shape[0]-len(Y))])\n",
        "            Y = np.concatenate((Y, y))\n",
        "    t2 = time.time()\n",
        "    print('time---- ', t2-t1)\n",
        "    return X_all, X_filter, Y"
      ],
      "metadata": {
        "id": "mQnUACNVCeqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all, X_filter, Y = get_data( nfiles = 12)\n",
        "\n",
        "np.savez('datasetBH', Xall=X_all, Xfilt=X_filter, Y=Y)\n"
      ],
      "metadata": {
        "id": "3quW9y_zCkBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "d9b52a1c-c3f2-43e3-ec15-3d6712981cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0c1a09332221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasetBH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXfilt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-82410b851870>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(nfiles)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             file = pd.read_csv('data/' + folder[i] + '/Dyn/' + metallicity[j] + '/' + name, \n\u001b[0;32m---> 17\u001b[0;31m                                engine='python', sep = ' ')\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"readline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/GC_chi01_output_noclusterevolv/Dyn/0.0002/nth_generation.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns without 'ignore them' but not bolded were also stored but they did not shown better performance."
      ],
      "metadata": {
        "id": "WYLXmOjiEZvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting"
      ],
      "metadata": {
        "id": "zwYhkdMzEHDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to visualize the main proprieties of the clusters and compare them, the dataframes is divided into some sub-dataframe collected into a dictionary. In this way it is possibile to recall a precise dataframe characterized by the cluster, metallicity and number of generation."
      ],
      "metadata": {
        "id": "2WSWGjgx4H8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify the visualization of some dependencies, sub-dataframes with a variance less than zero and with a number of counts lower than 10 were dropped. Furthermore, a filter has been performed on the spins, imposing that the spin magnitude of the black hole was at least 0.75."
      ],
      "metadata": {
        "id": "9g2B7LIA3uG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "octopus=dict() #Dictionary used to collect all the dataframes divided by generation, cluster and metallicity.\n",
        "#I choose dictionary and not an array because I don't know a priori the dimension\n",
        "whale=np.zeros((3, 4)) #Array to collect the max generation of each dataframe divided by cluster and metallicity\n",
        "moray=np.zeros((3, 4)) #Array that I will use to show the different max generations created by each cluster\n",
        "meta={0:3,1:10,2:5,3:11} #I choose only 4 metallicities to make the visualization simpler\n",
        "for c in range(0,3): #for every cluster: 0 is GC, 1 is NSC and 2 is YSC\n",
        "    squid=dict() #Dictionary used to collect the dataframes divided by generation and metallicity\n",
        "    for m in range(0,4):\n",
        "        df_plot=choose_set(c,meta[m])\n",
        "        #now I want to give columns a proper name:\n",
        "        df_plot= df_plot.set_axis([\"mass primary BH $(M_\\u2609)$\",\"mass secondary BH $(M_\\u2609)$\",\"spin magnitude primary BH\",\"spin magnitude secondary BH\",\"mass remnant BH $(M_\\u2609)$\",\"spin magnitude remnant BH\",\"escape velocity from cluster $(km/s^{-1})$\",\"total mass cluster $(M_\\u2609)$\",\"n of generation\"], axis=1)\n",
        "\n",
        "        #I divide the dataset by the number of generation\n",
        "\n",
        "        max_gen=df_plot['n of generation'].max() #I suppose that every set could have a different max generation\n",
        "        moray[c][m]=int(max_gen)\n",
        "        jellyfish=dict() #Dictionary to collect the filtred dataframes divided by generation\n",
        "        for i in range(2,max_gen+1):\n",
        "            df_i=df_plot[(df_plot['n of generation'] == i)]\n",
        "            #to visualize the main relations, a filter on the spins is made:\n",
        "            df_i=df_i[(df_i[\"spin magnitude remnant BH\"]>0.75)]\n",
        "            if df_i[\"mass primary BH $(M_\\u2609)$\"].var()>0 and df_i[\"mass primary BH $(M_\\u2609)$\"].count()>10: #I impose this condition because I don't want empty plots\n",
        "                jellyfish[i] = df_i\n",
        "        whale[c][m]=int(max(jellyfish.keys()))  #max of the filtred dictionaries for each metallicity\n",
        "        squid[m]=jellyfish\n",
        "    octopus[c]=squid"
      ],
      "metadata": {
        "id": "4HC1E7nQwDV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "59539ec9-dc51-47ab-aa81-5c3df52f1418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1e05e416d7d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msquid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Dictionary used to collect the dataframes divided by generation and metallicity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdf_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoose_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#now I want to give columns a proper name:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdf_plot\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mass primary BH $(M_\\u2609)$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mass secondary BH $(M_\\u2609)$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spin magnitude primary BH\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spin magnitude secondary BH\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mass remnant BH $(M_\\u2609)$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"spin magnitude remnant BH\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"escape velocity from cluster $(km/s^{-1})$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"total mass cluster $(M_\\u2609)$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n of generation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7735b4c61ad8>\u001b[0m in \u001b[0;36mchoose_set\u001b[0;34m(c, m)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#for this part is important to have the data stored on a folder called 'data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     file = pd.read_csv('data/' + folder[c] + '/Dyn/' + metallicity[m] + '/' + name, \n\u001b[0;32m---> 12\u001b[0;31m                    engine='python', sep = ' ')\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"readline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/GC_chi01_output_noclusterevolv/Dyn/0.0004/nth_generation.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **HISTOGRAMS**\n",
        "\n"
      ],
      "metadata": {
        "id": "SNn2S6rFyOU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The sub-dataframes are used to compare the different main features (mass, spin and velocity) between the different clusters. An additional comparison is made between generations and metallicity."
      ],
      "metadata": {
        "id": "zh-5t1Ndm1Ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MASSES**"
      ],
      "metadata": {
        "id": "BOooSl1krAcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first feature compared are the masses of the primary and secondary BH merged. This first analysis is made on basic values of metallicity (0,004) and for 2nd generation. The reason for this choice lies in the fact that we want to show only the general relations between the masses of the three clusters."
      ],
      "metadata": {
        "id": "zvguXAQGnSgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "starfish=whale.astype(int).T #I need to transpose the matrix because now the loops will be ordered differently\n",
        "#each row is the max number of generation of the three different cluster with the same metallicity\n",
        "colors=[\"skyblue\",\"crimson\",\"aquamarine\",\"royalblue\",\"salmon\",\"springgreen\",\"rebeccapurple\",\"firebrick\",\"yellow\",\"hotpink\",\"seagreen\",\"orchid\",\"gold\",\"coral\",\"forestgreen\"] #15\n",
        "clusters=[\"GC\",\"NSC\",\"YSC\"] #for cluster labels\n",
        "metall=[\"0,004\",\"0,0016\",\"0,006\",\"0,016\"] #for metallicity labels\n",
        "mean=np.zeros((3,4,5)) #to collect the medium values\n",
        "var=np.zeros((3,4,5)) #to collect the variances\n",
        "\n",
        "#I make a first plot to look at the general masses.\n",
        "#I fix the value of metallicity to 0.004 and I consider only the 2g for semplicity\n",
        "fig,(axm1,axm2)= plt.subplots(nrows=2, ncols=1,figsize=(10,6),sharex=True,sharey=True)\n",
        "fig.tight_layout()\n",
        "for c in range(0,3):\n",
        "    sns.kdeplot(octopus[c][0][2][\"mass primary BH $(M_\\u2609)$\"], shade=True, ax=axm1,label=\"primary 2g BH for \"+clusters[c],color=colors[c],alpha=0.5)\n",
        "    sns.kdeplot(octopus[c][0][2][\"mass secondary BH $(M_\\u2609)$\"], shade=True, ax=axm2,label=\"secondary 2g BH for \"+clusters[c],color=colors[c],alpha=0.5)\n",
        "    axm1.set_title('mass primary BH 2g for metallicity 0,004',fontsize = 15)\n",
        "    axm1.set_ylabel('Density')\n",
        "    axm1.legend(loc=\"best\")\n",
        "    axm2.set_title('mass secondary BH 2g for metallicity 0,004',fontsize = 15)\n",
        "    axm2.set_xlabel('Masses ($M_\\u2609$)')\n",
        "    axm2.legend(loc=\"best\")\n",
        "    axm1.grid(visible=True, axis='both', alpha=0.2)\n",
        "    axm2.grid(visible=True, axis='both', alpha=0.2)"
      ],
      "metadata": {
        "id": "9CpS1Knq6Sp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The masses of the primary and secondary black holes presents different values depending on the cluster. Nuclear and Globular clusters' ones are slightly smaller than the holes in the Young Star Clusters, but the two partially overlap."
      ],
      "metadata": {
        "id": "ktBjJrGUof1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in range(0,4):\n",
        "    sns.set_palette(colors)\n",
        "    fig1, ax1 = plt.subplots(figsize=(10,6))\n",
        "    for g in range(2,max(starfish[m])+1):#in this way the number of relevant plot it's decided\n",
        "        for c in range(0,3):\n",
        "            if starfish[m][c]<g: #to avoid the jump of generation if a cluster doesn't have one\n",
        "                break\n",
        "            sns.kdeplot(octopus[c][m][g][\"mass remnant BH $(M_\\u2609)$\"], shade=True, ax=ax1,label=\"BH \"+str(g)+\"g for \"+clusters[c],alpha=0.5)\n",
        "            ax1.set_title('Comparison of BH resultant masses for metallicity '+metall[m],fontsize = 15)\n",
        "            ax1.set_xlabel('Masses ($M_\\u2609$)')\n",
        "            ax1.set_ylabel('Density')\n",
        "            ax1.legend(loc=\"best\")\n",
        "            ax1.set_xticks(np.linspace(int(min(octopus[c][m][g][\"mass remnant BH $(M_\\u2609)$\"]))-200,int(max(octopus[c][m][g][\"mass remnant BH $(M_\\u2609)$\"]))+200,15))\n",
        "            ax1.grid(visible=True, axis='both', alpha=0.2)"
      ],
      "metadata": {
        "id": "7dFblvMuobR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous plots show that the remnant masses after the merging of the primary and secondary holes are characterized by the cluster, the metallicity and the generation.\n",
        "This last feature was quite obvious, however it can be seen how the resulting average mass increases generation by generation. With it, it increase also the variace, making the peaks more attenuated.\n",
        "Another immediate result comes from the comparison between different clusters. Nuclear Star Clusters produce the smaller masses, but on the other hand they give rise to a higher number of generations. The YSC, on the other hand, give rise to larger masses but fewer generations.\n",
        "A further comment can be done about metallicity: the mean values of the masses of all clusters divided by each generation decrease as the metallicity rise. This fact makes us think that the presence of elements other than hydrogen and helium in the clusters is relevant in the merging of binary black holes."
      ],
      "metadata": {
        "id": "cYvr4UbQ6mah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPINS**"
      ],
      "metadata": {
        "id": "mxcypyOsq93Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to the previous section, in this first analysis we want to visualize only the generic differences between the spins of black holes in different clusters."
      ],
      "metadata": {
        "id": "gBS3Y-DbrKZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figss,(axs1,axs2)= plt.subplots(nrows=2, ncols=1,figsize=(10,6),sharex=True,sharey=True)\n",
        "figss.tight_layout()\n",
        "for c in range(0,3):\n",
        "    sns.kdeplot(octopus[c][0][2][\"spin magnitude primary BH\"], shade=True, ax=axs1,label=\"primary 2g BH for \"+clusters[c],color=colors[c],alpha=0.5)\n",
        "    sns.kdeplot(octopus[c][0][2][\"spin magnitude secondary BH\"], shade=True, ax=axs2,label=\"secondary 2g BH for \"+clusters[c],color=colors[c],alpha=0.5)\n",
        "    axs1.set_title('spin primary BH 2g for metallicity 0,004',fontsize = 15)\n",
        "    axs1.set_ylabel('Density')\n",
        "    axs1.legend(loc=\"best\")\n",
        "    axs2.set_title('spin secondary BH 2g for metallicity 0,004',fontsize = 15)\n",
        "    axs2.set_xlabel('Spin magnitude')\n",
        "    axs2.legend(loc=\"best\")\n",
        "    axs1.grid(visible=True, axis='both', alpha=0.2)\n",
        "    axs2.grid(visible=True, axis='both', alpha=0.2)"
      ],
      "metadata": {
        "id": "H-GXY6D1q34-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be observed how, unlike the masses, in this case the spins of the first and second black holes are different from each other but similar for different clusters."
      ],
      "metadata": {
        "id": "2Lf-fPp5roNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in range(0,4):\n",
        "    fig2, ax2 = plt.subplots(figsize=(10,6))\n",
        "    for g in range(2,max(starfish[m])+1):#in this way the number of relevant plot it's decided\n",
        "        for c in range(0,3):\n",
        "            if starfish[m][c]<g: #to avoid the jump of generation if a cluster doesn't have one\n",
        "                break\n",
        "            octopus[c][m][g]=octopus[c][m][g][(octopus[c][m][g][\"spin magnitude remnant BH\"]>0.75)]\n",
        "            sns.kdeplot(octopus[c][m][g][\"spin magnitude remnant BH\"], shade=True, ax=ax2,label=\"BH \"+str(g)+\"g for \"+clusters[c],alpha=0.5)\n",
        "            #ax1.set_xticks(range(0,100))\n",
        "            ax2.set_title('Comparison of BH resultant spins for metallicity '+metall[m],fontsize = 15)\n",
        "            ax2.set_xlabel('Spin Magnitude')\n",
        "            ax2.set_ylabel('Density')\n",
        "            ax2.legend(loc=\"best\")\n",
        "            ax2.grid(visible=True, axis='both', alpha=0.2)"
      ],
      "metadata": {
        "id": "iokOvxs66rNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous plots we can see that initally, for 2g, the spins between the clusters are similar, similarly to the previous plot. When the number of generation increases, also the spin magnitude increases, even if it can be noted that the differences between 3 and 4 generations are small. In this case the metallicity acts slightly only in its higher values, reducing the spin, slowing down the BH."
      ],
      "metadata": {
        "id": "s0pfUsaf64CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ESCAPE VELOCITIES**"
      ],
      "metadata": {
        "id": "XNHScLK7sdnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As will be shown in the machine learing analysis, the most significant feature to distinguish the three clusters from each other is represented by the escape velocity."
      ],
      "metadata": {
        "id": "uYdfNqXwsjPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig444,axv1= plt.subplots(figsize=(10,6))\n",
        "for c in range(0,3):\n",
        "    sns.kdeplot(octopus[c][0][2][\"escape velocity from cluster $(km/s^{-1})$\"], shade=True, ax=axv1,label=\"escape velocity for \"+clusters[c],color=colors[c],alpha=0.5)\n",
        "    print(\"For \"+clusters[c]+\" the escape velocity is: \",octopus[c][0][2][\"escape velocity from cluster $(km/s^{-1})$\"].mean(),\"+/-\",octopus[c][0][2][\"escape velocity from cluster $(km/s^{-1})$\"].var(),\"km/s^{-1}\")\n",
        "    axv1.set_title('Escape velocities for metallicity 0,004, 2g BH',fontsize = 15)\n",
        "    axv1.set_ylabel('Density')\n",
        "    axv1.legend(loc=\"best\")\n",
        "    axv1.grid(visible=True, axis='both', alpha=0.2)"
      ],
      "metadata": {
        "id": "nloIfKLws4Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this plot it is already evident that the escape velocity is very different between the various clusters, moreover it is well defined for each of them (the variance is small) so it is reasonable to assume that this parameter can allow to uniquely distinguish the clusters between them."
      ],
      "metadata": {
        "id": "mFJXdOMZs_hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in range(0,4):\n",
        "    sns.set_palette(colors)\n",
        "    fig9, ax9 = plt.subplots(figsize=(10,6))\n",
        "    for g in range(2,max(starfish[m])+1):#in this way the number of relevant plot it's decided\n",
        "        for c in range(0,3):\n",
        "            if starfish[m][c]<g: #to avoid the jump of generation if a cluster doesn't have one\n",
        "                break\n",
        "            sns.kdeplot(octopus[c][m][g][\"escape velocity from cluster $(km/s^{-1})$\"], shade=True, ax=ax9,label=str(g)+\"g for \"+clusters[c],alpha=0.5)\n",
        "            ax9.set_title('Comparison of escape velocities for metallicity '+metall[m],fontsize = 15)\n",
        "            ax9.set_xlabel('Velocity $(km/s^{-1})$')\n",
        "            ax9.set_ylabel('Density')\n",
        "            ax9.legend(loc=\"best\")\n",
        "            ax9.set_xticks(np.linspace(int(min(octopus[c][m][g][\"escape velocity from cluster $(km/s^{-1})$\"]))-10,int(max(octopus[c][m][g][\"escape velocity from cluster $(km/s^{-1})$\"]))+10,5))\n",
        "            ax9.grid(visible=True, axis='both', alpha=0.2)"
      ],
      "metadata": {
        "id": "mAqruXTAvYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The very first observation that can be made is relative to the generations: once again the increase in generation corresponds to a higher value of the feature.\n",
        "Furthermore, again as proof of the fact that the escape velocities characterize the clusters well, it is observed that despite the advancement of the generations they do not get confused with those of the other clusters. Finally, it is noted that metallicity does not significantly affect this parameter."
      ],
      "metadata": {
        "id": "zBMMmtvgvbrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NUMBER OF GENERATIONS**"
      ],
      "metadata": {
        "id": "oGUPBAa3xPiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous plots it has been observed that the number of generations produced is not the same for each cluster. This aspect is quantified in the following representation."
      ],
      "metadata": {
        "id": "-uXIL7hTxdNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In this last histogram I want to show the differences between clusters\n",
        "fig3, ax3 = plt.subplots(figsize=(10,6))\n",
        "for i in range(0,3):\n",
        "    #print(moray[i])\n",
        "    sns.kdeplot(moray[i], shade=True, ax=ax3,alpha=0.5,label=clusters[i])\n",
        "    ax3.set_title('Max generation for each cluster',fontsize = 15)\n",
        "    ax3.set_xlabel('Generation')\n",
        "    ax3.set_ylabel('Density')\n",
        "    ax3.legend(loc=\"best\")\n",
        "    ax3.grid(visible=True, axis='both', alpha=0.2)"
      ],
      "metadata": {
        "id": "c375aSHX6iSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this plot it can be visualized the different max number of generations that each cluster can produce. As previously mentioned, nuclear clusters produce an higher number of generations, even if they produce smaller masses."
      ],
      "metadata": {
        "id": "TUHKPJlh7ANb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLUSTER MASSES**"
      ],
      "metadata": {
        "id": "cQX9Vn-9Q4jO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on to the scatter plots, one last significant feature is presented."
      ],
      "metadata": {
        "id": "URuMzXAIQ7rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig44,ax8= plt.subplots(figsize=(10,6))\n",
        "for c in range(0,3):\n",
        "    sns.kdeplot(octopus[c][0][2][\"total mass cluster $(M_\\u2609)$\"], shade=True, ax=ax8,label=\"total mass cluster for \"+clusters[c],color=colors[c],alpha=0.5)\n",
        "    print(\"For \"+clusters[c]+\" the total mass is: \",octopus[c][0][2][\"total mass cluster $(M_\\u2609)$\"].mean(),\"+/-\",octopus[c][0][2][\"total mass cluster $(M_\\u2609)$\"].var(),\"M_\\u2609\")\n",
        "    ax8.set_title('Total masses for metallicity 0,004, 2g BH',fontsize = 15)\n",
        "    ax8.set_ylabel('Density')\n",
        "    ax8.set_xticks(np.linspace(-1000000,7000000,10))\n",
        "    ax8.legend(loc=\"best\")\n",
        "    ax8.set_xlim(left=-1000000,right=7000000)\n",
        "    ax8.grid(visible=True, axis='both', alpha=0.2)\n",
        ""
      ],
      "metadata": {
        "id": "wibQRshxRN-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore the total mass cluster is a good indicator to distinguish young star clusters from the others two, but it does not clearly distinguish nuclear and globular from each other, since part of their masses are overlapping."
      ],
      "metadata": {
        "id": "gBFgdNzSSdKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **SCATTER PLOTS**\n",
        "\n"
      ],
      "metadata": {
        "id": "3aTw3dLsx228"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following plots some dependences between the features are shown. In order to do it, a single dataframe with all the meaningful data is created."
      ],
      "metadata": {
        "id": "GPBPxUoL7B0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dolphin=[0,0,0] #This time I use list and not dictionary to collect the dataframes outside the loops, because I want to concatenate all the dataframes\n",
        "for c in range(0,3):\n",
        "    seahorse=[] #list to collect all the dataframes except clusters and then concatenate them\n",
        "    for g in range(2,max(starfish[m])+1):\n",
        "        for m in range(0,4):\n",
        "            if starfish[m][c]<g: #to avoid problems\n",
        "                break\n",
        "            octopus[c][m][g][\"metallicity\"]=metall[m] #I add the column of the metallicity\n",
        "            seahorse.append(octopus[c][m][g]) #appending dataframes, so I can concatenate them\n",
        "    dolphin[c]=pd.concat(seahorse,ignore_index=True)\n",
        "    dolphin[c]=dolphin[c][(dolphin[c][\"spin magnitude primary BH\"]>0.5)]\n",
        "    dolphin[c]=dolphin[c][(dolphin[c][\"spin magnitude secondary BH\"]>0.5)]\n",
        "    dolphin[c]=dolphin[c][(dolphin[c][\"n of generation\"]<5)] #to have a better visualization of meaningful parameters\n",
        "    dolphin[c][\"cluster\"]=clusters[c] #I add the column of the cluster\n",
        "df=pd.concat(dolphin,ignore_index=True)\n",
        ""
      ],
      "metadata": {
        "id": "1QJDU3Fn7IWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MASSES**"
      ],
      "metadata": {
        "id": "rhZZaBmNzJgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme()\n",
        "\n",
        "sns.relplot(x=\"mass primary BH $(M_\\u2609)$\", y=\"mass remnant BH $(M_\\u2609)$\",  hue=\"cluster\",style=\"n of generation\",col=\"metallicity\", data=df,alpha=0.3);\n",
        "sns.relplot(x=\"mass secondary BH $(M_\\u2609)$\", y=\"mass remnant BH $(M_\\u2609)$\",  hue=\"cluster\",style=\"n of generation\",col=\"metallicity\", data=df,alpha=0.3);\n",
        ""
      ],
      "metadata": {
        "id": "TbDG46dgytxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking at these first set of plots it can be observed that there's a dipendence between the mass of the primary/secondary BH and the merged BH, but it becomes less accurate increasing the total mass. Anyway that's not surpirsing because it depends also on the mass of the secondary/primary BH. Young Star Clusters, which are lighter, are predictably concentrated near the origin. It can also be seen a confirmation that the overall masses decrease as the metallicity increases.\n"
      ],
      "metadata": {
        "id": "Yw23dFAdy8ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPINS**"
      ],
      "metadata": {
        "id": "rcUxGTEJzOZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.relplot(x=\"spin magnitude primary BH\", y=\"spin magnitude remnant BH\",  hue=\"cluster\",style=\"n of generation\",col=\"metallicity\", data=df,alpha=0.3);\n",
        "sns.relplot(x=\"spin magnitude secondary BH\", y=\"spin magnitude remnant BH\", hue=\"cluster\",style=\"n of generation\",col=\"metallicity\", data=df,alpha=0.3);"
      ],
      "metadata": {
        "id": "JMHVn7NZy2-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comparison of primary/secondary spin magitudes with the merged BH spin magnitude suggest that there may be a correlation between the two features, even if it is more defined for higher and lower spin magnitudes. Metallicity 0,016 suggests that for higher metallicity values the correlation could be more evident, but data are but the data are too few to be able to state it with certainty. Young star clusters have a smaller spin range, instead nuclear clsuters's range is wider.\n"
      ],
      "metadata": {
        "id": "ZAhs6q4UzUE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.relplot(y=\"escape velocity from cluster $(km/s^{-1})$\", x=\"total mass cluster $(M_\\u2609)$\", hue=\"cluster\",style=\"n of generation\",col=\"metallicity\", data=df,alpha=0.3);\n"
      ],
      "metadata": {
        "id": "U09ktv6Hy4kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the escape velocity has a logarithmic dependence on mass cluter. In addition, we can observe that a higher metallicity slightly decreases the escape velocities of the clusters. The Nuclear Cluters' velocities are considerably higher compared to the other two. In order to better visualize their characteristics, the following plot is restricted to globular and young clusters only.\n"
      ],
      "metadata": {
        "id": "CWWm03oqz0Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[(df[\"cluster\"]!=\"NSC\")]\n",
        "sns.relplot(y=\"escape velocity from cluster $(km/s^{-1})$\", x=\"total mass cluster $(M_\\u2609)$\", hue=\"cluster\",style=\"n of generation\",col=\"metallicity\", data=df,alpha=0.3);\n",
        ""
      ],
      "metadata": {
        "id": "P_hclHEUzxA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this last plot we can confirm the logarithmic relationship between the escape velocity and the masses of the clusters, as well as observe that the globular clusters escape faster than the young ones.\n",
        ""
      ],
      "metadata": {
        "id": "d8rTqflU016x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, some significant differences between the three clusters can be observed from the plots. First of all, the values ​​of the masses of black holes are lower for the Nuclear Star Clusters and higher for the Youngs. These values are also affected by metallicity and, of course, generation. The escape velocities, on the other hand, are very high for the nuclear ones and low for the young ones, a parameter that will be seen to be decisive for the differentiation between the clusters. A similar dependency can be found by looking at the number of generations produced by each cluster.\n",
        "Also the range of the magnitude of spins can be useful to characterize the cluster: in fact it is shown that nuclear cluster's one are larger compared to the other two, instead the young's one is significantly smaller."
      ],
      "metadata": {
        "id": "d43P5Hre2AQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA"
      ],
      "metadata": {
        "id": "4Hlykq30EEsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA comparison between different clusters\n",
        "\n",
        "Principal component analysis (PCA) is a technique that allows to reduce the dimensionality of a dataset. In particular, data described by multiple features, can be represented by a fewer number of dymensions, corresponding to the so called 'principal components', all orthogonal with each other. Each principal component catches a certain amount of variability in the dataset. For this reason, before the analysis, it is possible to fix a reasonable threshold of variance we want to be represented and see how many principal components are needed to represent the data in the most complete way.\n",
        "\n",
        "Below, after normalizig the data in order to reduce computation times, we used PCA on three datasets for a fixed metallicity and for each stellar cluster, with the only purpose of visualizing the data and seeing with a naked eye whether it is possible to distinguish three different clusters of points in the plots, corresponding to each stellar cluster. In order to do this, we chose the highest possible numbers of principal components that could be visualized (3 - 4). Furthermore, in order to have a quantitative estimation of how much the chosen numbers of principal components are representative of the original data, we also calculated the percentage of total represented variance."
      ],
      "metadata": {
        "id": "TNN1jOt1aJKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixing a metallicity\n",
        "met = 0\n",
        "\n",
        "#Data (features)\n",
        "X_GC = choose_set(0, met) #Globular cluster data\n",
        "X_NSC = choose_set(1, met) #Nuclear star cluster data\n",
        "X_YSC = choose_set(2, met) #Young star cluster data\n",
        "\n",
        "#Display dataframes\n",
        "#display(X_GC)\n",
        "#display(X_NSC)\n",
        "#display(X_YSC)\n",
        "\n",
        "#Data normalization to decrease computation time\n",
        "X_GC = (X_GC - np.mean(X_GC, axis=0))/np.std(X_GC, axis=0)\n",
        "X_NSC = (X_NSC - np.mean(X_NSC, axis=0))/np.std(X_NSC, axis=0)\n",
        "X_YSC = (X_YSC - np.mean(X_YSC, axis=0))/np.std(X_YSC, axis=0)\n",
        "\n",
        "#Display normalized dataframes\n",
        "#display(X_GC)\n",
        "#display(X_NSC)\n",
        "#display(X_YSC)\n",
        "\n",
        "#Check if mean is 0 and variance is 1\n",
        "#print(np.mean(X_GC, axis=0), np.std(X_GC, axis=0))\n",
        "#print(np.mean(X_NSC, axis=0), np.std(X_NSC, axis=0))\n",
        "#print(np.mean(X_YSC, axis=0), np.std(X_YSC, axis=0))"
      ],
      "metadata": {
        "id": "hxILZlC8EH1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA using 3 principal components, applied to each cluster separately and resume main features"
      ],
      "metadata": {
        "id": "nnM1CHdV5pRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=3)\n",
        "principalComponents_GC = pca.fit_transform(X_GC)\n",
        "principalDf_GC = pd.DataFrame(data = principalComponents_GC, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
        "print(\"Globular cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 3 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "principalComponents_NSC = pca.fit_transform(X_NSC)\n",
        "principalDf_NSC = pd.DataFrame(data = principalComponents_NSC, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
        "print(\"Nuclear star cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 3 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "principalComponents_YSC = pca.fit_transform(X_YSC)\n",
        "principalDf_YSC = pd.DataFrame(data = principalComponents_YSC, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
        "print(\"Young star cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 3 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')"
      ],
      "metadata": {
        "id": "2jvwPwpUEMRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of PCA using 3 principal components"
      ],
      "metadata": {
        "id": "83AR7Xxa50pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_GC = np.array(principalDf_GC['principal component 1'])\n",
        "y_GC = np.array(principalDf_GC['principal component 2'])\n",
        "z_GC = np.array(principalDf_GC['principal component 3'])\n",
        "\n",
        "x_NSC = np.array(principalDf_NSC['principal component 1'])\n",
        "y_NSC = np.array(principalDf_NSC['principal component 2'])\n",
        "z_NSC = np.array(principalDf_NSC['principal component 3'])\n",
        "\n",
        "x_YSC = np.array(principalDf_YSC['principal component 1'])\n",
        "y_YSC = np.array(principalDf_YSC['principal component 2'])\n",
        "z_YSC = np.array(principalDf_YSC['principal component 3'])\n",
        "\n",
        "fig = plt.figure(figsize=(13, 12))\n",
        "fig.suptitle('3 components PCA', fontsize = 20)\n",
        "ax1 = fig.add_subplot(221, projection='3d')\n",
        "ax1.scatter(x_GC, y_GC, z_GC, c='r', s = 5, alpha=0.3)\n",
        "ax1.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax1.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax1.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax1.set_title('Globular cluster', fontsize = 15)\n",
        "\n",
        "ax2 = fig.add_subplot(222, projection='3d')\n",
        "ax2.scatter(x_NSC, y_NSC, z_NSC, c='g', s = 5, alpha=0.3)\n",
        "ax2.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax2.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax2.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax2.set_title('Nuclear star cluster', fontsize = 15)\n",
        "\n",
        "ax3 = fig.add_subplot(223, projection='3d')\n",
        "ax3.scatter(x_YSC, y_YSC, z_YSC, c='b', s = 5, alpha=0.3)\n",
        "ax3.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax3.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax3.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax3.set_title('Young star cluster', fontsize = 15)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SsU_g82NESDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA using 4 principal components, applied to each cluster separately and resume main features"
      ],
      "metadata": {
        "id": "Fmm35wRZ54WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=4)\n",
        "principalComponents_GC = pca.fit_transform(X_GC)\n",
        "principalDf_GC = pd.DataFrame(data = principalComponents_GC, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])\n",
        "print(\"Globular cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 4 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')\n",
        "\n",
        "pca = PCA(n_components=4)\n",
        "principalComponents_NSC = pca.fit_transform(X_NSC)\n",
        "principalDf_NSC = pd.DataFrame(data = principalComponents_NSC, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])\n",
        "print(\"Nuclear star cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 4 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')\n",
        "\n",
        "pca = PCA(n_components=4)\n",
        "principalComponents_YSC = pca.fit_transform(X_YSC)\n",
        "principalDf_YSC = pd.DataFrame(data = principalComponents_YSC, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])\n",
        "print(\"Young star cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 4 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')"
      ],
      "metadata": {
        "id": "SMRaVTanEWou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of PCA using 4 principal components"
      ],
      "metadata": {
        "id": "EPNV_1N859JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_GC = np.array(principalDf_GC['principal component 1'])\n",
        "y_GC = np.array(principalDf_GC['principal component 2'])\n",
        "z_GC = np.array(principalDf_GC['principal component 3'])\n",
        "c_GC = np.array(principalDf_GC['principal component 4'])\n",
        "\n",
        "x_NSC = np.array(principalDf_NSC['principal component 1'])\n",
        "y_NSC = np.array(principalDf_NSC['principal component 2'])\n",
        "z_NSC = np.array(principalDf_NSC['principal component 3'])\n",
        "c_NSC = np.array(principalDf_NSC['principal component 4'])\n",
        "\n",
        "x_YSC = np.array(principalDf_YSC['principal component 1'])\n",
        "y_YSC = np.array(principalDf_YSC['principal component 2'])\n",
        "z_YSC = np.array(principalDf_YSC['principal component 3'])\n",
        "c_YSC = np.array(principalDf_YSC['principal component 4'])\n",
        "\n",
        "fig = plt.figure(figsize=(13, 10))\n",
        "fig.suptitle('4 components PCA', fontsize = 20)\n",
        "ax1 = fig.add_subplot(221, projection='3d')\n",
        "img1 = ax1.scatter(x_GC, y_GC, z_GC, c=c_GC, cmap=plt.hot(), s = 5, alpha=0.3)\n",
        "fig.colorbar(img1, label='Principal Component 4')\n",
        "ax1.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax1.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax1.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax1.set_title('Globular cluster', fontsize = 15)\n",
        "\n",
        "ax2 = fig.add_subplot(222, projection='3d')\n",
        "img2 = ax2.scatter(x_NSC, y_NSC, z_NSC, c=c_NSC, cmap=plt.hot(), s = 5, alpha=0.3)\n",
        "fig.colorbar(img2, label='Principal Component 4')\n",
        "ax2.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax2.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax2.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax2.set_title('Nuclear star cluster', fontsize = 15)\n",
        "\n",
        "ax3 = fig.add_subplot(223, projection='3d')\n",
        "img3 = ax3.scatter(x_YSC, y_YSC, z_YSC, c=c_YSC, cmap=plt.hot(), s = 5, alpha=0.3)\n",
        "fig.colorbar(img3, label='Principal Component 4')\n",
        "ax3.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax3.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax3.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax3.set_title('Young star cluster', fontsize = 15)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ImiEDtJDEbcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 3 and 4 principal components we can see that the accuracy is not very high, in fact the total represented variance is around 76% for 3 principal components description and around 86% for 4. However, this analysis was only meant for visualization purposes."
      ],
      "metadata": {
        "id": "SKFd-difa6Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA using 6 principal components, applied to each cluster separately and resume main features"
      ],
      "metadata": {
        "id": "py4haaIQ6Dsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=6)\n",
        "principalComponents_GC = pca.fit_transform(X_GC)\n",
        "principalDf_GC = pd.DataFrame(data = principalComponents_GC, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5', 'principal component 6'])\n",
        "print(\"Globular cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 6 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')\n",
        "\n",
        "pca = PCA(n_components=6)\n",
        "principalComponents_NSC = pca.fit_transform(X_NSC)\n",
        "principalDf_NSC = pd.DataFrame(data = principalComponents_NSC, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5', 'principal component 6'])\n",
        "print(\"Nuclear star cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 6 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')\n",
        "\n",
        "pca = PCA(n_components=6)\n",
        "principalComponents_YSC = pca.fit_transform(X_YSC)\n",
        "principalDf_YSC = pd.DataFrame(data = principalComponents_YSC, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5', 'principal component 6'])\n",
        "print(\"Young star cluster:\")\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 6 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data\\n')"
      ],
      "metadata": {
        "id": "Rxw2rtlXEe98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 6 principal components, instead, we are able to represent about 96% of the total variability, which is a reasonably accurate result, however, difficult to visualize.\n",
        "\n",
        "In principle, we could now train a clustering algorithm in order to assign every collection of features, described by the principal components, to the right stellar cluster. However, this would not give any information on which are the original features that influences the most on the fate of a binary black hole in the three different kinds of star clusters. In fact it is not possible to directly link the information contained in the principal components, back to the original features in the dataset."
      ],
      "metadata": {
        "id": "dMveTLZlbD3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA used to \"clusterize\" BH mergers generations inside a choosen stellar cluster"
      ],
      "metadata": {
        "id": "Iz0kGzA-ElSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we used again the PCA, but in a different way. Here we visualized the data for a fixed star cluster and metallicity, and then 'clusterized' the numbers of generations of the mergers. The purpose consists in investigating wether it is possible to discriminate between different generations for a same cluster and metallicity. As was told us from prof. Mapelli, we should expect to be barely able to distinguish the second generation from the others."
      ],
      "metadata": {
        "id": "CmDDOEvAbOhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixing a metallicity and a stellar cluster\n",
        "met = 0\n",
        "clus = 0\n",
        "\n",
        "#Data\n",
        "df_filt=choose_set(clus, met)\n",
        "\n",
        "X = np.array(df_filt.drop(columns = ['c27']) ) #Features\n",
        "X = (X - np.mean(X, axis=0))/np.std(X, axis=0) #Features normalization in order to decrease computation time\n",
        "#X = StandardScaler().fit_transform(X) #Alternative way of normalizing data\n",
        "Y = np.array( df_filt['c27'] ) #Labels (merger BH generation)\n",
        "\n",
        "#Check if mean is 0 and variance is 1\n",
        "#print(np.mean(X, axis=0))\n",
        "#print(np.std(X, axis=0))"
      ],
      "metadata": {
        "id": "2WpLQWjs0wfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA using 3 principal components, applied to the features of a single cluster and resume main properties"
      ],
      "metadata": {
        "id": "29bIT6gqbkB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=3)\n",
        "principalComponents = pca.fit_transform(X)\n",
        "\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 3 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data')"
      ],
      "metadata": {
        "id": "TrlQY1kgXjzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of PCA using 3 principal components"
      ],
      "metadata": {
        "id": "yxbEnj6abxHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(principalDf['principal component 1'])\n",
        "y = np.array(principalDf['principal component 2'])\n",
        "z = np.array(principalDf['principal component 3'])\n",
        "\n",
        "classes = np.unique(Y)\n",
        "colors = np.array(['r', 'g', 'b', 'c', 'm', 'y', 'orange', 'gray', 'pink', 'brown', 'salmon', 'chocolate', 'lawngreen', 'lightgreen', 'darkcyan', 'steelblue', 'magenta'])\n",
        "leg = np.array([])\n",
        "for val in classes: leg = np.append(leg, str(val) + ' gen')\n",
        "\n",
        "fig = plt.figure(figsize(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax.set_title('3 components PCA', fontsize = 15)\n",
        "\n",
        "for classes, colors in zip(classes, colors[0:len(classes)]):\n",
        "    indicesToKeep = Y == classes\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1'], principalDf.loc[indicesToKeep, 'principal component 2'], principalDf.loc[indicesToKeep, 'principal component 3'], c=colors, s = 5, alpha=0.3)\n",
        "\n",
        "plt.legend(leg ,prop={'size': 7}, loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H-cXaUbzXuRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA using 4 principal components, applied to the features of a single cluster and resume main properties"
      ],
      "metadata": {
        "id": "zYkUbEqRb5g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=4)\n",
        "principalComponents = pca.fit_transform(X)\n",
        "\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])\n",
        "print('Fraction of variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Using 4 principal components we are able to represent the ', np.sum(pca.explained_variance_ratio_) * 100, '% of the variance of the data')"
      ],
      "metadata": {
        "id": "G6jfzYQhX5Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of PCA using 4 principal components"
      ],
      "metadata": {
        "id": "3zaEv663cBcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(principalDf['principal component 1'])\n",
        "y = np.array(principalDf['principal component 2'])\n",
        "z = np.array(principalDf['principal component 3'])\n",
        "c = np.array(principalDf['principal component 4'])\n",
        "\n",
        "classes = np.unique(Y)\n",
        "colors = np.array(['r', 'g', 'b', 'c', 'm', 'y', 'orange', 'gray', 'pink', 'brown', 'salmon', 'chocolate', 'lawngreen', 'lightgreen', 'darkcyan', 'steelblue', 'magenta'])\n",
        "leg = np.array([])\n",
        "for val in classes: leg = np.append(leg, str(val) + ' gen')\n",
        "h = np.array([])\n",
        "l = np.array([])\n",
        "\n",
        "fig = plt.figure(figsize(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 7)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 7)\n",
        "ax.set_zlabel('Principal Component 3', fontsize = 7)\n",
        "ax.set_title('4 components PCA', fontsize = 15)\n",
        "img = ax.scatter(x, y, z, c=c, cmap=plt.hot(), alpha=0.4, s=5) # Plot data points\n",
        "fig.colorbar(img, label='Principal Component 4')\n",
        "\n",
        "for classes, colors in zip(classes, colors[0:len(classes)]):\n",
        "    cnt = 0\n",
        "    indicesToKeep = Y == classes\n",
        "    points = np.array((principalDf.drop(columns = 'principal component 4')).loc[indicesToKeep])\n",
        "    if len(points) > 3:\n",
        "        hull = ConvexHull(points) # Get convex hull\n",
        "        for s in hull.simplices:\n",
        "            s = np.append(s, s[0]) # Repeat last point to close the polygon\n",
        "            ax.plot(points[s, 0], points[s, 1], points[s, 2], '-', c=colors, linewidth=0.5, alpha=0.4, label=(str(classes)+'gen')) # Plot 'cluster' lines\n",
        "        h = np.append(h, np.array(ax.get_legend_handles_labels())[:, -1][0])\n",
        "        l = np.append(l, np.array(ax.get_legend_handles_labels())[:, -1][1])\n",
        "    else:\n",
        "        ax.plot(points[:, 0], points[:, 1], points[:, 2], '-', c=colors, linewidth=0.5, alpha=0.4, label=(str(classes)+'gen')) # Plot 'cluster' lines\n",
        "        h = np.append(h, np.array(ax.get_legend_handles_labels())[:, -1][0])\n",
        "        l = np.append(l, np.array(ax.get_legend_handles_labels())[:, -1][1])\n",
        "    cnt += 1\n",
        "\n",
        "fig.legend(h, l, loc='upper left', title='Legend')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "coE4Gmc2VQev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing clus=0 and met=0, we see that second generation mergers are well distinguishable from the others. On the contrary, choosing for exaple clus=1 and met=5, clustering for second generation mergers become difficult if not imposible to appreciate."
      ],
      "metadata": {
        "id": "DIgqUvuScKQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, we can observe that PCA was certainly useful to have an intuition of how data are distributed in function of a fixed number of principal components. However, the difficulty of linking the visualized results to the original features, suggests the use of a more suitable machine learning algorithm, in order to understand which are the features of the data that impact the most on the fate of a binary black hole on the three stellar clusters."
      ],
      "metadata": {
        "id": "kP-FKDtYweDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##RANDOM FOREST\n",
        "\n",
        "What are random Forests?\n",
        "\n",
        "Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification problems. It builds decision trees on different samples and takes their majority vote for classification. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain. Since decisions trees are very sensitive to the data they are trained on (small changes to the training set can result in significantly different tree structures), random forest allow each individual tree to randomly sample from the dataset with replacement, resulting in different trees. This process is known as bagging. Furthermore: each tree in a random forest can pick only from a random subset of features. This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification.\n"
      ],
      "metadata": {
        "id": "1wzJ85mUDYme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we upload out data, normalize it and split it into train and test set"
      ],
      "metadata": {
        "id": "pwfc9bGdffoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xns = np.load('datasetBH.npz')\n",
        "X = xns['Xfilt']\n",
        "Y = xns['Y']"
      ],
      "metadata": {
        "id": "g8M8fRXS7zq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once our data is uploaded, it is important to normalize it. The normalization of the data has a huge impact on the performance of machine learning algorithms. Non normalizing it could make the algorithm to not converge while fitting the data or even not beeing able to fit it."
      ],
      "metadata": {
        "id": "z6XH34vPFSO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ID=4607431\n",
        "np.random.seed(ID) #we define a seed to permute out data\n",
        "\n",
        "def split_norm( X, Y):\n",
        "    #split the data into train and test set (70%-30%)\n",
        "    m = X.shape[0]\n",
        "    m_train = int(0.7*m)\n",
        "    m_test = m - m_train\n",
        "    #split and permute the data (train-test)\n",
        "    permutation = np.random.permutation(m) # random permutation\n",
        "    X = X[permutation,:]\n",
        "    Y = Y[permutation]\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = m_test )\n",
        "    #normalizing the data\n",
        "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test) # use the same transformation on test data\n",
        "\n",
        "    print(X_train[:10]) #we print 10 samples of to have a look on the dataset.\n",
        "    return X_train,X_test ,Y_train ,Y_test\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = split_norm(X, Y)\n",
        "\n",
        "\n",
        "labels, freqs = np.unique(Y_train, return_counts=True)\n",
        "print(\"Labels in training dataset: \", labels)\n",
        "print(\"Frequencies in training dataset: \", freqs)"
      ],
      "metadata": {
        "id": "MYvfsgFl77sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to have a look on the frequencies of our labels, as we can see they are not homogeneous which can lead the model to be biased by the predominant label while giving a high score over the test set. This way we can now take that into account while creating our model."
      ],
      "metadata": {
        "id": "UkL9zHJ3Gb3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the whole dataset we can take a small dataset to look for the best hyperparameters that later will conform our model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SKbC__QmGvlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_training = 10000\n",
        "permutation = np.random.permutation(m_training)\n",
        "X_train = X_train[permutation,:]\n",
        "Y_train = Y_train[permutation]\n",
        "X_train = X_train[:m_training]\n",
        "Y_train = Y_train[:m_training]\n",
        "labels, freqs = np.unique(Y_train, return_counts=True)\n",
        "print(\"Labels in training dataset: \", labels)\n",
        "print(\"Frequencies in training dataset: \", freqs)"
      ],
      "metadata": {
        "id": "2Nad1C2iGuoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try first to see the performance of a very simple forest and the most important features that it predicts.\n",
        "Now we are using a random forest characterized by 10 decision trees (n_estimators) and balanced class_weight is a parameter that takes in account the strong asymmetry between number of samples in each cluster."
      ],
      "metadata": {
        "id": "WThOhnp98Heb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators=10, class_weight = 'balanced')\n",
        "forest.fit(X_train, Y_train)\n",
        "preds = forest.predict(X_test)\n",
        "col_names = ['c0','c1', 'c2','c3', 'c4','c9','c13','c15','c16', 'c17', 'c25','c27', 'c28' ]\n",
        "\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
        "\n",
        "plt.bar(col_names,forest.feature_importances_, yerr=std) #the importance of every feature\n",
        "plt.title(\"Random Forest Feature Importance using MDI\",fontsize=20 )\n",
        "plt.ylabel(\"Mean decrease in impurity\", fontsize=15)\n",
        "#plt.tight_layout()\n",
        "\n"
      ],
      "metadata": {
        "id": "9B5HMelzDkmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can organize this preliminary results on a more visual way"
      ],
      "metadata": {
        "id": "z2ME3GCafKXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to have a better vision of feature importance\n",
        "sorted_idx = forest.feature_importances_.argsort()\n",
        "#print(sorted_idx)\n",
        "col_names=np.array(col_names)\n",
        "std = np.array(std)\n",
        "plt.bar(col_names[sorted_idx], forest.feature_importances_[sorted_idx], yerr=std[sorted_idx])\n",
        "plt.title(\"Random Forest Feature Importance using MDI\",fontsize=20 )\n",
        "plt.ylabel(\"Mean decrease in impurity\", fontsize=15)\n",
        "#plt.tight_layout()"
      ],
      "metadata": {
        "id": "t1oJFxVQWTd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The blue bars are the feature importances of the forest, along with their inter-trees variability represented by the error bars. As it can be noticed column 9, time requested for the dynamical pair up of the binary black hole in\n",
        "Myr, column 25, total mass of the stellar cluster in Msun, and column 17, escape velocity from the star cluster, are the most significant ones in the decision process; but since column 2, and column 13 have great variability they can also be taken in account, while the remaining ones result to be not informative."
      ],
      "metadata": {
        "id": "40Wa-9kbq87M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate performance of the predictor it can be shown the the mean accuracy on the train and test data and labels, which coresponds to 1 - training error, and 1 - test error."
      ],
      "metadata": {
        "id": "o7faYzd9tS8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a way to evaluate random forest\n",
        "print(\"training score:\", forest.score(X_train, Y_train))\n",
        "print(\"test score:    \", forest.score(X_test, Y_test))"
      ],
      "metadata": {
        "id": "XnoyBW8nX4l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance seem to be accurate but, since our data is not homogeneus, a more visual way to check the performance of the model is with a confusin matrix since the model could be biased by the predominant label and still have a good score.\n",
        "By definition a confusion matrix C is such that C_i,j is equal to the number of observations known to be in group i  and predicted to be in group j, so in the principal diagonal we can see the number of correctly classified samples, and in the external position we have the misclassified samples.\n"
      ],
      "metadata": {
        "id": "0jdh2RRXgSXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#confusion matrix shows improvements but still overfitting\n",
        "\n",
        "print(confusion_matrix(Y_train, forest.predict(X_train)))\n",
        "print(confusion_matrix(Y_test, preds))"
      ],
      "metadata": {
        "id": "uxrafSKBPOBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe, this models perform correctly on the train set but it generalize bad to the test set which mean that it is overfitted. To solve this problem we have to look for a set of hyperparameters that allow our model perform over the test set correctly."
      ],
      "metadata": {
        "id": "w7lRMCnbvWQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STUDY OF THE HYPERPARAMETERS INDIVIDUALLY"
      ],
      "metadata": {
        "id": "AJZUYvS4hw2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the previous classiefier we have studied took in account only default values of internal parameters, now we can study the individual performance of each parameter, which we can be consider as hyperparameters, to see the impact that each one may have on the results, taking also into account the computational time."
      ],
      "metadata": {
        "id": "oA_t4F7XhErb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to create a grid search\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [100, 300, 500, 800]\n",
        "# Number of features to consider at every split\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [5, 15, 25, 50]\n",
        "# Minimum number of samples required to split a node\n",
        "\n",
        "min_samples_split = [ 10, 15, 20, 30, 40 , 50, 60, 70, 80]\n",
        "# Minimum number of samples required at each leaf node\n",
        "\n",
        "# Criterion\n",
        "criterion=['gini', 'entropy'] #entropy always lead to better results\n"
      ],
      "metadata": {
        "id": "oWDlet52_TZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where the previous default values of each parameter were:\n",
        "* n_estimators = 100.\n",
        "* max_depth = None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "* min_sample_split = 5.\n",
        "* criterion = 'gini'.\n",
        "\n",
        "Gini is a function to evaluate the purity of each split. Instead entropy give us information about gain of a split.\n",
        "\n",
        "All the parameters for the gridsearch are defined on the upper cell.\n",
        "\n",
        "Here we studied how total number of trees impact on performances."
      ],
      "metadata": {
        "id": "ZN-6BDt-iEnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WARNING, RUNNING THIS CELL TAKES REALLY LONG TIME!!  ---->more or less 5 minutes\n",
        "\n",
        "params = {'n_estimators': n_estimators}\n",
        "RF = RandomForestClassifier(class_weight = 'balanced', criterion = 'entropy')\n",
        "\n",
        "#the 'balanced acuracy' force the gridsearch to have into acount the asymetry of the dataset.\n",
        "#otherwhise the score can be easily biased by the predominants labels.\n",
        "grid = GridSearchCV(estimator = RF, param_grid = params, scoring = 'balanced_accuracy',\n",
        "                   return_train_score = True, verbose = 2)\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Best parameters set found:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "print(\"Score with best parameters:\")\n",
        "print(grid.best_score_)\n",
        "\n",
        "train_list=grid.cv_results_['mean_train_score']\n",
        "test_list=grid.cv_results_['mean_test_score']\n",
        "\n",
        "fit_time = grid.cv_results_['mean_fit_time']"
      ],
      "metadata": {
        "id": "SfHs1md2h37m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, n_e = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "n_e[0].scatter(n_estimators, train_list , label = 'train')\n",
        "n_e[0].set_xlabel('nº of trees: ' )\n",
        "n_e[0].set_ylabel('mean_score')\n",
        "n_e[0].scatter(n_estimators, test_list, label = 'test')\n",
        "n_e[0].set_title('number of trees impact')\n",
        "n_e[0].legend()\n",
        "\n",
        "n_e[1].scatter(n_estimators, fit_time, label = 'time')\n",
        "n_e[1].set_xlabel('nº of trees')\n",
        "n_e[1].set_ylabel('cpu time')\n",
        "n_e[1].legend()"
      ],
      "metadata": {
        "id": "KljZUsRaiQgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see in general there is no significant dependence on total number of trees, instead computational time seems to increase consistently, as expected. In conclusion we can choose a standard value of n_estimator=100, which consist in a good compromise for computational time, as best hyperparameter.\n",
        "\n",
        "Here we studied how the maximum depth of each tree impact on performances."
      ],
      "metadata": {
        "id": "CaJ-n1f_k0iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'max_depth': max_depth}\n",
        "RF = RandomForestClassifier(class_weight = 'balanced', criterion = 'entropy')\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator = RF, param_grid = params, scoring = 'balanced_accuracy',\n",
        "                   return_train_score = True, verbose = 2)\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Best parameters set found:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "print(\"Score with best parameters:\")\n",
        "print(grid.best_score_)\n",
        "\n",
        "train_list=grid.cv_results_['mean_train_score']\n",
        "test_list=grid.cv_results_['mean_test_score']\n",
        "\n",
        "fit_time = grid.cv_results_['mean_fit_time']"
      ],
      "metadata": {
        "id": "8YJ5x_xuiXWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, md = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "md[0].scatter(max_depth, train_list , label = 'train')\n",
        "md[0].set_xlabel('Max_depth: ' )\n",
        "md[0].set_ylabel('mean_score')\n",
        "md[0].scatter(max_depth, test_list, label = 'test')\n",
        "md[0].set_title('max_depth impact')\n",
        "md[0].legend()\n",
        "\n",
        "md[1].scatter(max_depth, fit_time, label = 'time')\n",
        "md[1].set_xlabel('Max_depth')\n",
        "md[1].set_ylabel('cpu time')\n",
        "md[1].legend()"
      ],
      "metadata": {
        "id": "mFelioDLibjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph shows that as max_depth increases the train score tends to saturate instead test score seems to stabilize to a certain value near 0.800, since this is behaviour is interesting, we,ve decided to study more in detail this parameter.\n",
        "\n",
        "Here we studied how the minimun number of leafs required to split impact on performances."
      ],
      "metadata": {
        "id": "PMZfXPs1nR6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'min_samples_split': min_samples_split}\n",
        "RF = RandomForestClassifier( class_weight = 'balanced', criterion = 'entropy')\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator = RF, param_grid = params, scoring = 'balanced_accuracy',\n",
        "                   return_train_score = True, verbose = 2)\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Best parameters set found:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "print(\"Score with best parameters:\")\n",
        "print(grid.best_score_)\n",
        "\n",
        "train_list=grid.cv_results_['mean_train_score']\n",
        "test_list=grid.cv_results_['mean_test_score']\n",
        "\n",
        "fit_time = grid.cv_results_['mean_fit_time']"
      ],
      "metadata": {
        "id": "c3G7qjMmiqrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ms_s = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "ms_s[0].scatter(min_samples_split, train_list , label = 'train')\n",
        "ms_s[0].set_xlabel('samples-split: ' )\n",
        "ms_s[0].set_ylabel('mean_score')\n",
        "ms_s[0].scatter(min_samples_split, test_list, label = 'test')\n",
        "ms_s[0].set_title('min-samples-split impact')\n",
        "ms_s[0].legend()\n",
        "\n",
        "ms_s[1].scatter(min_samples_split, fit_time, label = 'time')\n",
        "ms_s[1].set_xlabel('samples-split')\n",
        "ms_s[1].set_ylabel('cpu time')\n",
        "ms_s[1].legend()"
      ],
      "metadata": {
        "id": "KBOGxzXrivzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also in this case we have an interesting behaviour: as min_samples_split increases both train and test score seems to stabilize on a common value, so we decided to go into detail even for this parameter.\n",
        "\n",
        "Once we have seen which parameters has more impact in terms of accuracy and computational time we can do a bigger gridsearch to get a more accurate model."
      ],
      "metadata": {
        "id": "BsiRcOWuAJKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'max_depth': max_depth, 'min_samples_split': min_samples_split}\n",
        "RF = RandomForestClassifier( class_weight = 'balanced', criterion = 'entropy')\n",
        "\n",
        "grid = GridSearchCV(estimator = RF, param_grid = params, scoring = 'balanced_accuracy',\n",
        "                   return_train_score = True, verbose = 2)\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Best parameters set found:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "print(\"Score with best parameters:\")\n",
        "print(grid.best_score_)"
      ],
      "metadata": {
        "id": "PR8N149uAW80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final check to the performance of the best hyperparameters founded could be increasing the size of the dataset."
      ],
      "metadata": {
        "id": "Bk18dXK8hYVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#once we have the best of parameters choosen we try adding more data\n",
        "\n",
        "m_t = [10000, 20000, 30000]\n",
        "tempo = [] #cpu-time for each number of samples\n",
        "scor = [] #score over the test set\n",
        "for i in m_t:\n",
        "    X_train, X_test, Y_train, Y_test = split_norm(X, Y)\n",
        "    permutation = np.random.permutation(i)\n",
        "    X_train = X_train[permutation,:]\n",
        "    Y_train = Y_train[permutation]\n",
        "    X_train = X_train[:i]\n",
        "    Y_train = Y_train[:i]\n",
        "\n",
        "    t1 = time.time()\n",
        "    R = RandomForestClassifier(class_weight = 'balanced', criterion = 'entropy',\n",
        "                               min_samples_split = grid.best_params_['min_samples_split'],\n",
        "                               max_depth = grid.best_params_['max_depth'])\n",
        "    R.fit(X_train, Y_train)\n",
        "    t2 = time.time()\n",
        "    tempo.append(t2-t1)\n",
        "    scor.append(R.score(X_test, Y_test))"
      ],
      "metadata": {
        "id": "FlhMYOFXi2bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, s = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "s[0].scatter(m_t, scor , label = 'test')\n",
        "s[0].set_xlabel('nº of samples: ' )\n",
        "s[0].set_ylabel('mean_score')\n",
        "s[0].legend()\n",
        "\n",
        "s[1].scatter(m_t, tempo, label = 'time')\n",
        "s[1].set_xlabel('nº of samples')\n",
        "s[1].set_ylabel('cpu time')\n",
        "s[1].legend()"
      ],
      "metadata": {
        "id": "WGKc7t12mXVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, increasing size of dataset could be a good option till 20000 samples, beyond that value, performances doesn't improve significantly. So we are safe to say that a using larger set of samples represent just a time consuming operation."
      ],
      "metadata": {
        "id": "2ag8u3ParzCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL MODEL"
      ],
      "metadata": {
        "id": "VbgcMuQnBJ1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have founded the best combinations of hyperparameters we can study the performance of the model and compare it to the simple forest that we saw at the begining."
      ],
      "metadata": {
        "id": "M9eGgQAKhxK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = split_norm(X, Y)\n",
        "\n",
        "m =20000 # best number of samples founded avobe\n",
        "permutation = np.random.permutation(m)\n",
        "X_train = X_train[permutation,:]\n",
        "Y_train = Y_train[permutation]\n",
        "X_train = X_train[:m]\n",
        "Y_train = Y_train[:m]"
      ],
      "metadata": {
        "id": "WxGbD2hviZHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForestClassifier(class_weight = 'balanced', criterion = 'entropy',\n",
        "                          # n_estimators = grid.best_params_['n_estimators'],\n",
        "                           min_samples_split = grid.best_params_['min_samples_split'],\n",
        "                           max_depth = grid.best_params_['max_depth'])\n",
        "\n",
        "RF.fit(X_train, Y_train)\n",
        "\n",
        "print ('score over the train set: ',RF.score(X_train, Y_train))\n",
        "print('score over the test set: ',RF.score(X_test, Y_test))\n",
        "\n",
        "\n",
        "std = np.std([tree.feature_importances_ for tree in RF.estimators_], axis=0)\n",
        "\n",
        "plt.bar(col_names,RF.feature_importances_, yerr=std) #the importance of every feature\n",
        "plt.title(\"Random Forest Feature Importance using MDI\",fontsize=20 )\n",
        "plt.ylabel(\"Mean decrease in impurity\", fontsize=15)\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "GadY86UMPSo4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e295e68d-da88-448f-e0e7-b701960e7f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0ab9019f1ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m RF = RandomForestClassifier(class_weight = 'balanced', criterion = 'entropy', \n\u001b[0m\u001b[1;32m      2\u001b[0m                           \u001b[0;31m# n_estimators = grid.best_params_['n_estimators'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                            \u001b[0mmin_samples_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_samples_split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            max_depth = grid.best_params_['max_depth'])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to have a better vision of feature importance\n",
        "sorted_idx = RF.feature_importances_.argsort()\n",
        "col_names=np.array(col_names)\n",
        "\n",
        "std = np.array(std)\n",
        "plt.bar(col_names[sorted_idx], RF.feature_importances_[sorted_idx], yerr=std[sorted_idx])\n",
        "plt.title(\"Random Forest Feature Importance using MDI\",fontsize=20 )\n",
        "plt.ylabel(\"Mean decrease in impurity\", fontsize=15)\n",
        "#plt.tight_layout()"
      ],
      "metadata": {
        "id": "NlHZ_91_kZ06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_train = (skm.ConfusionMatrixDisplay.from_predictions(Y_train, RF.predict(X_train)))\n",
        "plt.title('Confusion matrix for train data ( ' + str(len(Y_train)) + ' samples)')\n",
        "\n",
        "conf_test = (skm.ConfusionMatrixDisplay.from_predictions(Y_test, RF.predict(X_test)))\n",
        "plt.title('Confusion matrix for test data ( '+ str(len(Y_test))+' samples)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XxspwodzBRoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL CONCLUSIONS"
      ],
      "metadata": {
        "id": "BRhucC6r5G8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We studied the main differences of binary black hole systems from different star clusters. After plotting the main properties of those systems we can state that the features that better characterize the three stellar clusters are the masses of the black holes which can be found in it, the escape velocity and the number of generations produced. Contrary to the masses, the escape velocities remain similar in each cluster even for the black holes of further generations and they are not affected by metallicity, therefore they are a more significant parameter to differentiate the clusters.\n",
        "\n",
        "In addition, we developed a machine learning algorithm to see if it was able to predict whether if each system belongs to one or another type of star cluster. The algorithm choosen was Random Forest, after adjusting some of its hyperparameters and trainigng different models with different sizes of dataset, we saw that it was able to classify correctly the problem even taking into account the assymetry of the data.\n",
        "\n",
        "On the other hand, in contrast with the study realized by plotting the main parameters of each system the algorithm predicted that the most relevant feature to determine the type of cluster the system belong to was the escape velocity of the system.\n",
        "This make sense because as the cluster become bigger, the stars that try to escape it will feel more atraction towards the center of masses of the cluster which will reduce its escape valocity.\n",
        "\n",
        "In particular, even changing parameters and reducing the overfitting, the prediction on three most reevant feature doesn't change, and this result to be in agreement with graphical prediction from plots."
      ],
      "metadata": {
        "id": "VYItJytZJVpl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7D6rrTxutbcm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}